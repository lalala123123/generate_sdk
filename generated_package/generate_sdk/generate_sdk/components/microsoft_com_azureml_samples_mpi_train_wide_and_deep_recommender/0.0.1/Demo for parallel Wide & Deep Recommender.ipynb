{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Register modules\n",
    "\n",
    "Follow [this guide](https://msdata.visualstudio.com/AzureML/_git/ModuleDocs?path=%2Fdocs%2Fcli%2Fa-quick-go-through.md&_a=preview) to install the Azure Machine Learning CLI for module registration;\n",
    "\n",
    "Reigster the following 3 custom modules:\n",
    "* https://github.com/hirofumi-s-friends/CustomModules/blob/master/azureml-custom-module-examples/wide-and-deep-recommender/train_mpi.yaml\n",
    "* https://github.com/hirofumi-s-friends/CustomModules/blob/master/azureml-custom-module-examples/wide-and-deep-recommender/score_parallel.yaml\n",
    "* https://github.com/hirofumi-s-friends/CustomModules/blob/master/azureml-custom-module-examples/wide-and-deep-recommender/convert_multi_parquet_to_dfd.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment, Workspace, Dataset\n",
    "from azureml.core import Environment\n",
    "from azureml.core.runconfig import CondaDependencies\n",
    "from azureml.pipeline.core import Pipeline, PipelineData, Module\n",
    "from azureml.pipeline.steps import ModuleStep\n",
    "from azureml.contrib.pipeline.steps import ParallelRunStep, ParallelRunConfig\n",
    "from azureml.pipeline.core import PipelineParameter\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace_name=\"Your workspace name\"\n",
    "resource_group_name=\"Your resource group name\"\n",
    "subscription_id=\"Your subscription id\"\n",
    "compute_name = \"Your compute name\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.get(name=workspace_name, resource_group=resource_group_name, subscription_id=subscription_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "found existing compute target.\nAzure Machine Learning Compute attached\n"
    }
   ],
   "source": [
    "try:\n",
    "    compute = AmlCompute(ws, compute_name)\n",
    "    print(\"found existing compute target.\")\n",
    "except ComputeTargetException:\n",
    "    print(\"creating new compute target\")\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_NC6\",\n",
    "                                                                min_nodes = 1, \n",
    "                                                                max_nodes = 4)    \n",
    "    compute = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "    compute.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "print(\"Azure Machine Learning Compute attached\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get or create input Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_file_dataset(ws, name, local_folder):\n",
    "    try:\n",
    "        return Dataset.get_by_name(ws, name=name)\n",
    "    except:\n",
    "        path_on_datastore = f'/data/{name}'\n",
    "        datastore = ws.get_default_datastore()\n",
    "        datastore.upload(str(local_folder), target_path=path_on_datastore)\n",
    "        datastore_paths = [(datastore, path_on_datastore + '/**')]\n",
    "        dataset = Dataset.File.from_files(path=datastore_paths)\n",
    "        dataset.register(ws, name=name, create_new_version=True)\n",
    "        return Dataset.get_by_name(ws, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_train = get_or_create_file_dataset(ws, name='train_dfd', local_folder='./sample_data/train_data_frame_directory').as_named_input('data').as_mount()\n",
    "data_to_score = get_or_create_file_dataset(ws, name='test_pq', local_folder='./sample_data/test_parquet_files').as_named_input('data').as_mount()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define pipeline data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = PipelineData(name='trained_model', datastore=ws.get_default_datastore())\n",
    "scored_dfd = PipelineData(name='score_output', datastore=ws.get_default_datastore())\n",
    "test_dfd = PipelineData(name='test_dfd', datastore=ws.get_default_datastore())\n",
    "eval_result = PipelineData(name='eval_result', datastore=ws.get_default_datastore())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_module(ws, name, namespace):\n",
    "    \"\"\"A method to get the module from the specific namespace in the workspace. \"\"\"\n",
    "    return Module.get(ws, name=f\"{namespace}://{name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define MPI train module step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpi_train_module = get_module(ws, name=\"MPI Train Wide and Deep Recommender\",namespace='microsoft.com/azureml/samples')\n",
    "mpi_train_step = ModuleStep(module=mpi_train_module,\n",
    "                            inputs_map={'Training_dataset_of_user_item_rating_triples': data_to_train},\n",
    "                            outputs_map={'Trained_Wide_and_Deep_recommendation_model': trained_model},\n",
    "                            params={'Epochs': 12,\n",
    "                                      'Batch size': 64,\n",
    "                                      'Wide part optimizer': 'Adagrad',\n",
    "                                      'Wide optimizer learning rate': 0.1,\n",
    "                                      'Crossed feature dimension': 1000,\n",
    "                                      'Deep part optimizer': 'Adagrad',\n",
    "                                      'Deep optimizer learning rate': 0.1,\n",
    "                                      'User embedding dimension': 16,\n",
    "                                      'Item embedding dimension': 16,\n",
    "                                      'Categorical features embedding dimension': 4,\n",
    "                                      'Hidden units': '256,128',\n",
    "                                      'Activation function': 'ReLU',\n",
    "                                      'Dropout': 0.8,\n",
    "                                      'Batch Normalization': 'True',\n",
    "                                      'NodeCount': 2,\n",
    "                                      'MpiProcessCountPerNode': 1,\n",
    "                                      'Arguments': 'USE_STRUCTURED_ARGUMENTS'},\n",
    "                            compute_target=compute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define parallel score module step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_score_module = get_module(ws, name=\"Parallel Score Wide and Deep Recommender\",namespace='microsoft.com/azureml/samples')\n",
    "parallel_score_step = ModuleStep(module=parallel_score_module,\n",
    "                                 inputs_map={'Trained_Wide_and_Deep_recommendation_model': trained_model,\n",
    "                                             'Dataset_to_score':data_to_score},\n",
    "                                 outputs_map={'Scored_dataset':scored_dfd},\n",
    "                                 params={'Recommender prediction kind':'Rating Prediction',\n",
    "                                         'Arguments': 'USE_STRUCTURED_ARGUMENTS'},\n",
    "                                 compute_target=compute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define multi parquets convert module step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_parquet_module = get_module(ws, \n",
    "                                    name='Convert Multi Parquet Files to DataFrameDirectory', \n",
    "                                    namespace='microsoft.com/azureml/samples')\n",
    "convert_parquet_step = ModuleStep(\n",
    "    module=convert_parquet_module,\n",
    "    inputs_map={'Input_path': data_to_score},\n",
    "    outputs_map={'Output_path': test_dfd},\n",
    "    params={'Arguments': 'USE_STRUCTURED_ARGUMENTS'},\n",
    "    compute_target=compute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define evaluate recommender module step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_recommender_module = get_module(ws, name='Evaluate Recommender', namespace='azureml')\n",
    "evaluate_recommender_step = ModuleStep(\n",
    "    module=evaluate_recommender_module,\n",
    "    inputs_map={'Scored_dataset': scored_dfd, 'Test_dataset': test_dfd},\n",
    "    outputs_map={'Metric': eval_result},\n",
    "    params={'Arguments': 'USE_STRUCTURED_ARGUMENTS'},\n",
    "    compute_target=compute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(ws, steps=[\n",
    "        mpi_train_step,\n",
    "        parallel_score_step,\n",
    "        convert_parquet_step,\n",
    "        evaluate_recommender_step,\n",
    "    ])\n",
    "exp = Experiment(ws, name='wide_and_deep_recommender').submit(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python36864bit3ec5a8e8ae43404082b741548ead2968"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}